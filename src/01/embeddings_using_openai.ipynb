{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Embeddings using OpenAI\n",
    "\n",
    "<div align=\"left\">\n",
    "  <a href=\"https://colab.research.google.com/github/simonguest/dp-applied-genai/blob/main/src/01/embeddings_using_openai.ipynb\" target=\"blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "  </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI's Text Embeddings\n",
    "\n",
    "OpenAI provides state-of-the-art text embedding models that convert text into high-dimensional vectors. These embeddings capture semantic meaning and can be used for various tasks like:\n",
    "\n",
    "- **Semantic search**: Finding similar documents or passages\n",
    "- **Clustering**: Grouping similar texts together\n",
    "- **Classification**: Using embeddings as features for ML models\n",
    "- **Recommendation systems**: Finding similar content\n",
    "\n",
    "In this notebook, we'll explore OpenAI's `text-embedding-ada-002` model, which produces 1536-dimensional vectors and is optimized for both quality and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and API Key\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "1. An OpenAI API key (set as environment variable `OPENAI_API_KEY`)\n",
    "2. The OpenAI Python library installed: `pip install openai`\n",
    "\n",
    "**Note**: Using OpenAI's API incurs costs. The embedding model is relatively inexpensive, but be mindful of your usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Your First Embedding\n",
    "\n",
    "Let's start by generating an embedding for a simple text string. The embedding will be a list of 1536 floating-point numbers that represent the semantic meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "print(f\"Embedding dimensions: {len(response.data[0].embedding)}\")\n",
    "print(f\"First 10 values: {response.data[0].embedding[:10]}\")\n",
    "print(f\"Full embedding: {response.data[0].embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "**Try this**: Change the input text in the cell above and run it again. Notice how:\n",
    "- The embedding always has 1536 dimensions\n",
    "- Different texts produce different embeddings\n",
    "- The values are typically between -1 and 1\n",
    "\n",
    "**Question to consider**: What happens when you use the exact same text twice? Do you get identical embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Embeddings: Semantic Similarity\n",
    "\n",
    "The real power of embeddings comes from comparing them. We can measure how similar two pieces of text are by calculating the cosine similarity between their embeddings.\n",
    "\n",
    "**Cosine similarity** ranges from -1 to 1:\n",
    "- 1 = identical meaning\n",
    "- 0 = no relationship\n",
    "- -1 = opposite meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Generate embeddings for two similar sentences\n",
    "response = client.embeddings.create(\n",
    "    input=[\"The cat sat on the mat.\",\n",
    "           \"A feline rested on a rug.\"],\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "embedding_a = response.data[0].embedding\n",
    "embedding_b = response.data[1].embedding\n",
    "\n",
    "# Calculate cosine similarity (1 - cosine distance)\n",
    "similarity_score = 1 - cosine(embedding_a, embedding_b)\n",
    "print(f\"Cosine similarity: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Text Pairs\n",
    "\n",
    "**Try these experiments** by modifying the input texts in the cell above:\n",
    "\n",
    "1. **Synonymous sentences**: \n",
    "   - \"The dog is running\" vs \"The canine is jogging\"\n",
    "   \n",
    "2. **Related but different topics**:\n",
    "   - \"I love pizza\" vs \"Italian food is delicious\"\n",
    "   \n",
    "3. **Completely unrelated**:\n",
    "   - \"The weather is sunny\" vs \"Mathematics is challenging\"\n",
    "   \n",
    "4. **Opposite meanings**:\n",
    "   - \"I am happy\" vs \"I am sad\"\n",
    "\n",
    "**Questions to explore**:\n",
    "- What similarity scores do you get for each pair?\n",
    "- Do the scores align with your intuition about semantic similarity?\n",
    "- How does this compare to simple word matching?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Multiple Texts\n",
    "\n",
    "For efficiency, you can generate embeddings for multiple texts in a single API call. This is more cost-effective and faster than individual calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple texts to embed\n",
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on a rug.\",\n",
    "    \"Dogs are loyal companions.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"Machine learning is fascinating.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all texts\n",
    "response = client.embeddings.create(\n",
    "    input=texts,\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = [data.embedding for data in response.data]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Similarity Matrix\n",
    "\n",
    "Let's create a matrix showing the similarity between all pairs of texts. This helps visualize which texts are most similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate similarity matrix\n",
    "n_texts = len(embeddings)\n",
    "similarity_matrix = np.zeros((n_texts, n_texts))\n",
    "\n",
    "for i in range(n_texts):\n",
    "    for j in range(n_texts):\n",
    "        if i == j:\n",
    "            similarity_matrix[i][j] = 1.0  # Perfect similarity with itself\n",
    "        else:\n",
    "            similarity_matrix[i][j] = 1 - cosine(embeddings[i], embeddings[j])\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity_matrix, \n",
    "    index=[f\"Text {i+1}\" for i in range(n_texts)],\n",
    "    columns=[f\"Text {i+1}\" for i in range(n_texts)]\n",
    ")\n",
    "\n",
    "print(\"Similarity Matrix:\")\n",
    "print(similarity_df.round(3))\n",
    "\n",
    "print(\"\\nOriginal texts:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Questions\n",
    "\n",
    "Looking at the similarity matrix above:\n",
    "\n",
    "1. **Which two texts are most similar?** Why do you think this is?\n",
    "2. **Which texts are least similar?** Does this make sense semantically?\n",
    "3. **How do the similarity scores compare to what you would expect intuitively?**\n",
    "\n",
    "**Advanced exploration**: Try adding more texts to the list and see how the similarity patterns change. Consider texts from different domains (sports, technology, cooking, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "From this exploration, you should understand:\n",
    "\n",
    "1. **Embeddings are dense vector representations** of text that capture semantic meaning\n",
    "2. **OpenAI's embeddings are high-quality** and can detect semantic similarity even when words are different\n",
    "3. **Cosine similarity** is a standard way to measure how similar two embeddings are\n",
    "4. **Batch processing** is more efficient for multiple texts\n",
    "5. **Embeddings enable many AI applications** like search, recommendation, and classification\n",
    "\n",
    "**Next steps**: Try using these embeddings in a real application, such as building a semantic search system or a text classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

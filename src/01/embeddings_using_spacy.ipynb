{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Embeddings using spaCy\n",
    "\n",
    "<div align=\"left\">\n",
    "  <a href=\"https://colab.research.google.com/github/simonguest/dp-applied-genai/blob/main/src/01/embeddings_using_spacy.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "  </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Vector Embeddings?\n",
    "\n",
    "Vector embeddings are numerical representations that map complex data—such as words, sentences, or images—into a continuous, high-dimensional vector space. This transformation enables machines to capture and process semantic relationships and contextual meanings inherent in the data. For instance, in natural language processing (NLP), word embeddings position semantically similar words closer together in the vector space, facilitating tasks like sentiment analysis, machine translation, and information retrieval.\n",
    "\n",
    "In this notebook, we'll use **spaCy**, a popular Python library for NLP that includes pre-trained word embeddings. spaCy's embeddings are based on word2vec and are trained on large text corpora to capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Dependencies\n",
    "\n",
    "Run the following cell to download the English pipeline (medium) optimized for CPU. This model includes:\n",
    "- **Word vectors**: 300-dimensional embeddings for ~685k words\n",
    "- **Part-of-speech tagging**: Grammatical information\n",
    "- **Named entity recognition**: Identifying people, places, organizations, etc.\n",
    "- **Dependency parsing**: Understanding sentence structure\n",
    "\n",
    "You can find more information here: https://spacy.io/models/en#en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Let's start by generating embeddings for a simple sentence. spaCy automatically creates document-level embeddings by averaging the word vectors in the text.\n",
    "\n",
    "**Try this**: Run the code below, then try with other words, sentences, or paragraphs. What do you notice about the embedding dimensions and values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process the word/sentence\n",
    "doc = nlp(\"The cat sat on the mat.\")\n",
    "\n",
    "# Display information about the embeddings\n",
    "print(f\"Text: '{doc.text}'\")\n",
    "print(f\"Embedding dimensions: {len(doc.vector)}\")\n",
    "print(f\"First 10 values: {doc.vector[:10]}\")\n",
    "print(f\"\\nFull embedding: {doc.vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word vs Document Embeddings\n",
    "\n",
    "spaCy provides embeddings at different levels:\n",
    "- **Token (word) level**: Each individual word has its own embedding\n",
    "- **Document level**: The entire text gets a single embedding (average of word embeddings)\n",
    "\n",
    "Let's explore both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sentence\n",
    "doc = nlp(\"The cat sat on the mat.\")\n",
    "\n",
    "print(\"Word-level embeddings:\")\n",
    "for token in doc:\n",
    "    if token.has_vector:  # Check if the word has an embedding\n",
    "        print(f\"'{token.text}': {len(token.vector)} dimensions, first 5 values: {token.vector[:5]}\")\n",
    "    else:\n",
    "        print(f\"'{token.text}': No embedding available\")\n",
    "\n",
    "print(f\"\\nDocument-level embedding: {len(doc.vector)} dimensions\")\n",
    "print(f\"Document vector (first 10): {doc.vector[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Similarity\n",
    "\n",
    "We can use spaCy's built-in `similarity` function to generate a similarity score between two pieces of text. This function calculates the cosine similarity between the embeddings.\n",
    "\n",
    "**Similarity scores range from 0 to 1:**\n",
    "- 1 = identical meaning\n",
    "- 0 = no relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process the sentences\n",
    "doc1 = nlp(\"The cat sat on the mat.\")\n",
    "doc2 = nlp(\"A feline rested on a rug.\")\n",
    "\n",
    "# Compute similarity\n",
    "similarity_score = doc1.similarity(doc2)\n",
    "\n",
    "print(f\"Text 1: '{doc1.text}'\")\n",
    "print(f\"Text 2: '{doc2.text}'\")\n",
    "print(f\"Similarity score: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic vs Semantic Similarity\n",
    "\n",
    "What do you notice about the similarity scores? Are they capturing syntactic (word structure) or semantic (meaning) similarity? \n",
    "\n",
    "**Experiment**: Re-run the previous cell with different sentence pairs to find out. Try these examples:\n",
    "\n",
    "1. **Same words, different order**: \n",
    "   - \"The dog chased the cat\" vs \"The cat chased the dog\"\n",
    "   \n",
    "2. **Synonyms**:\n",
    "   - \"The car is fast\" vs \"The automobile is quick\"\n",
    "   \n",
    "3. **Different topics**:\n",
    "   - \"I love programming\" vs \"The weather is nice\"\n",
    "\n",
    "**Questions to consider**:\n",
    "- Do sentences with similar meanings but different words get high similarity scores?\n",
    "- Do sentences with the same words but different meanings get high scores?\n",
    "- What does this tell you about what the embeddings are capturing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Multiple Texts\n",
    "\n",
    "Let's create a more comprehensive comparison by looking at multiple texts and their pairwise similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple texts to compare\n",
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A feline rested on a rug.\",\n",
    "    \"Dogs are loyal pets.\",\n",
    "    \"The weather is sunny today.\",\n",
    "    \"Programming is fun and challenging.\"\n",
    "]\n",
    "\n",
    "# Process all texts\n",
    "docs = [nlp(text) for text in texts]\n",
    "\n",
    "# Create similarity matrix\n",
    "print(\"Pairwise Similarity Scores:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc1 in enumerate(docs):\n",
    "    for j, doc2 in enumerate(docs):\n",
    "        if i < j:  # Only show upper triangle to avoid duplicates\n",
    "            similarity = doc1.similarity(doc2)\n",
    "            print(f\"Text {i+1} vs Text {j+1}: {similarity:.4f}\")\n",
    "            print(f\"  '{doc1.text}' vs '{doc2.text}'\")\n",
    "            print()\n",
    "\n",
    "print(\"\\nText Reference:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level Similarity Analysis\n",
    "\n",
    "Let's also explore similarity at the word level to understand how individual words relate to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare individual words\n",
    "words = [\"cat\", \"feline\", \"dog\", \"canine\", \"car\", \"automobile\", \"happy\", \"joyful\", \"sad\"]\n",
    "\n",
    "# Process words\n",
    "word_docs = [nlp(word) for word in words]\n",
    "\n",
    "print(\"Word Similarity Examples:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Compare some interesting word pairs\n",
    "pairs = [\n",
    "    (\"cat\", \"feline\"),\n",
    "    (\"dog\", \"canine\"), \n",
    "    (\"car\", \"automobile\"),\n",
    "    (\"happy\", \"joyful\"),\n",
    "    (\"happy\", \"sad\"),\n",
    "    (\"cat\", \"dog\"),\n",
    "    (\"cat\", \"car\")\n",
    "]\n",
    "\n",
    "for word1, word2 in pairs:\n",
    "    doc1 = nlp(word1)\n",
    "    doc2 = nlp(word2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    print(f\"'{word1}' vs '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "From this exploration with spaCy embeddings, you should understand:\n",
    "\n",
    "1. **spaCy provides 300-dimensional word embeddings** trained on large text corpora\n",
    "2. **Document embeddings are averages** of the individual word embeddings\n",
    "3. **Similarity scores capture semantic relationships** between words and texts\n",
    "4. **Embeddings work at multiple levels** - individual words, sentences, and documents\n",
    "5. **Pre-trained models** like spaCy's make it easy to get started with embeddings\n",
    "\n",
    "**Comparison with other approaches**: \n",
    "- spaCy embeddings are based on older techniques (word2vec) but are fast and work well for many tasks\n",
    "- Modern transformer-based models (like OpenAI's embeddings) often provide better semantic understanding\n",
    "- The choice depends on your specific use case, computational resources, and accuracy requirements\n",
    "\n",
    "**Next steps**: Try experimenting with different types of text (technical documents, creative writing, news articles) to see how well the embeddings capture domain-specific relationships!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
